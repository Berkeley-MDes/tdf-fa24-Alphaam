### **Progress Report**

**Reflections:**

- **What I learned:**  
  This week, I explored the fundamentals of using the ZeroWidth workbench and began understanding how to structure an LLM-based knowledge interface. Initially, I focused on familiarizing myself with the Retrieval Augmented Generation (RAG) framework, a method that injects context into the LLM session, allowing me to customize responses based on specific knowledge inputs. I discovered the potential of LLMs to interpret and generate responses based on personal journal entries, which showed me how personalized interactions with an LLM could enhance my project.

- **How I learned it:**  
  To build a foundation, I reviewed tutorials and video resources provided in the course. I set up a few test prompts on ZeroWidth, observing how information I inputted influenced the output. I also engaged in trial and error with several entries from my course journal, tweaking context settings to see how the LLM would incorporate them. The most challenging part was deciding what kind of information (whole paragraphs or key sentences) worked best to accurately represent my knowledge and style in responses.

- **Assessment of the state of work:**  
  By the end of the week, I’d set up an initial structure for the interface, which could retrieve weekly GitHub report entries and compare them with project deliverables. However, I encountered issues in ensuring that my prompts accurately represented weekly goals and provided relevant suggestions for improvements. I also started thinking about how to incorporate feedback mechanisms directly into the interface, aiming for a system that would give helpful insights on each report submission.

**Speculations:**

- **Future directions for tools:**  
  With the rapidly advancing LLM tools, I believe future iterations could offer greater customization, making it easier to incorporate feedback mechanisms. There’s a clear potential to develop more intuitive prompt engineering tools, which would allow designers to more accurately align LLM outputs with user needs. I envision these developments helping designers produce faster, more focused analyses.

- **Future directions for work:**  
  Next week, I hope to refine the feedback mechanism so that the system can provide more granular suggestions on specific sections of each report. Additionally, I want to explore options for standardizing prompts or including pre-set options to help streamline the user experience.

**Bonus: Interesting Industry Update:**
- **Description:**  
  I read an article on how OpenAI is expanding its partnership with Microsoft, integrating more customizable options into tools like Azure OpenAI. The article emphasized the potential for industries to leverage this technology in ways that are both user-specific and data-sensitive.
  - **Reference:** [OpenAI and Microsoft expand LLM capabilities](https://www.example.com/article)  
  - **Image:** Screenshot of article with a highlighted section discussing interface customization possibilities.

**Images & Video:**
- **Images**: 
    1. Screenshot of the initial ZeroWidth workbench setup with prompt input fields.
    2. Diagram showing the flow of information from GitHub report submissions to the feedback interface.

**Sketches, Drawings, and Diagrams:**


![](https://github.com/Berkeley-MDes/tdf-fa24-Alphaam/blob/main/Week%209/media/Screenshot%202024-11-06%20at%202.06.57%20PM.png)
![](https://github.com/Berkeley-MDes/tdf-fa24-Alphaam/blob/main/Week%209/media/Screenshot%202024-11-06%20at%202.33.33%20PM.png)
![](https://github.com/Berkeley-MDes/tdf-fa24-Alphaam/blob/main/Week%209/media/Screenshot%202024-11-07%20at%2012.59.24%20PM.png)

Initial experiments exploring the interaction between the GitHub report, RAG framework, and the feedback loop.



