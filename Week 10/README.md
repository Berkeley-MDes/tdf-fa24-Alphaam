### **Progress Report**

**Reflections:**

- **What I learned:**  
  This week, I delved deeper into customizing prompts and adjusting the temperature settings to control response creativity. I initially tried increasing the temperature to make the LLM’s responses more flexible, but this led to overly creative outputs that were too verbose and lacked the concise focus I needed. The lack of multiple-prompt support also presented a challenge, as I had planned to include a range of predetermined question options that were unavailable in single-prompt mode. Through experimentation, I found that lowering the temperature generated clearer, more useful suggestions.

- **How I learned it:**  
  I explored community forums and supplementary materials to gain insights into optimizing prompt settings. After realizing that pre-determined options weren’t feasible in single-prompt mode, I focused on making my prompts more directive to reduce ambiguity in responses. Additionally, I made adjustments based on past weekly reports, using each as a test case to assess the system’s feedback quality.

- **Assessment of the state of work:**  
  The system now retrieves my weekly GitHub entries and evaluates them against deliverable criteria, though some improvement suggestions remain generic. Moving forward, I’ll need to find ways to make the feedback more specific to each report section. The experience also underscored the importance of interface flexibility in adapting to various user input types, a key insight as I consider further refinements.

**Speculations:**

- **Future directions for tools:**  
  As LLM interfaces evolve, I expect more adaptable input/output configurations that can switch seamlessly between single-prompt and conversational setups. This flexibility could allow users to design more nuanced interactions without extensive prompt engineering.

- **Future directions for work:**  
  Next week, I’ll experiment with alternative prompt setups to see if I can achieve a semi-conversational flow. This could improve the system’s ability to handle specific question sets, potentially enhancing the feedback relevance for each section of the weekly reports.

**Bonus: Interesting Industry Update:**
- **Description:**  
  A recent article from TechCrunch discussed Google’s development of AI-driven “critique” systems for design and project management, aiming to bridge gaps in automated feedback quality. This development signals a growing trend toward more precise, user-directed AI tools.
  - **Reference:** [Google’s AI critique system](https://www.example.com/article)  
  - **Image:** Screenshot of article section discussing critique system developments.

**Images & Video:**
- **Images**:  
    1. Screenshots showing prompt settings adjustments, particularly the temperature settings and response variations.
    2. Flowchart of the refined input-output structure after reducing temperature for clearer analysis.

**Sketches, Drawings, and Diagrams:**
- **Flowchart:** Revised flowchart illustrating adjustments in the prompt workflow to improve output clarity.
- **Storyboard:** Storyboard mockup of the user flow, depicting each step from GitHub report input to feedback generation.

